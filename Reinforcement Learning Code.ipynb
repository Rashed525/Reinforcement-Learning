{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Betting Problem ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0.0020656234668683703, 0.00516405955015177, 0.009225470448896328, 0.012910153549983894, 0.017385398346403475, 0.023063677258169702, 0.027814111652448004, 0.03227538679658753, 0.037685072768208996, 0.04346349694116772, 0.050354469867537444, 0.057659194117779394, 0.06523937408012104, 0.0695352822693378, 0.0744312390078421, 0.08068846699146882, 0.08661104366092541, 0.09421268192052248, 0.10314362444807264, 0.10865874340470527, 0.11596662619655526, 0.1258861746688436, 0.13357997571793317, 0.1441479854307599, 0.16000000000000003, 0.16309843573009108, 0.16774609212999037, 0.17383820635490188, 0.17936523207795257, 0.18607809816470067, 0.19459551647066767, 0.20172116936160273, 0.20841308019488133, 0.2165276091523135, 0.2251952460428232, 0.2355317048013062, 0.24648879125845596, 0.2578590614380547, 0.2643029238129412, 0.27164685889882045, 0.28103270161696164, 0.2899165654913881, 0.3013190228807837, 0.3147154368628329, 0.3229881153392923, 0.33394993929483285, 0.3488292621176997, 0.36036996357689977, 0.3762219781461399, 0.4, 0.4030984357300911, 0.40774609212999036, 0.41383820635490187, 0.4193652320779525, 0.42607809816470066, 0.43459551647066763, 0.4417211693616027, 0.44841308019488135, 0.4565276091523135, 0.4651952460428232, 0.4755317048013062, 0.48648879125845595, 0.4978590614380547, 0.5043029238129412, 0.5116468588988204, 0.5210327016169617, 0.5299165654913881, 0.5413190228807837, 0.5547154368628329, 0.5629881153392923, 0.5739499392948328, 0.5888292621176997, 0.6003699635768998, 0.6162219781461399, 0.64, 0.6446476552779943, 0.6516191392467716, 0.6607573098824007, 0.6690478481169289, 0.6791171476256939, 0.6918932747550737, 0.7025817542877647, 0.712619620970177, 0.7247914137284702, 0.7377928692035753, 0.7532975572706199, 0.769733186887684, 0.7867885931667966, 0.7964543859294404, 0.8074702885754164, 0.8215490525726588, 0.8348748482370821, 0.8519785343623719, 0.872073155900078, 0.8844821731452498, 0.9009249089422493, 0.9232438935400468, 0.9405549453653496, 0.9643329672192097, 0]\n"
     ]
    }
   ],
   "source": [
    "\"Value Iteration for Sports Betting\"\n",
    "\"Value iteration helps generate v* (optimal value function) and pi* (optimal policy function)\"\n",
    "\n",
    "\"Discount factor\"\n",
    "gamma = 1\n",
    "\"Probability of home team winning\"\n",
    "p = 0.4\n",
    "\"The number of states availabe\"\n",
    "numStates = 100\n",
    "\"List for storing the reward value\"\n",
    "reward = [0 for _ in range(101)]\n",
    "reward[100]=1\n",
    "\"Small threshold value for comparing the difference\"\n",
    "theta = 0.00000001\n",
    "\n",
    "\"List to store the value function for all states form 1 to 99\"\n",
    "value=[0 for _ in range(101)]\n",
    "\"List to store the amount of bet that gives the max reward\"\n",
    "policy = [0 for _ in range(101)]\n",
    "\n",
    "def reinforcement_learning():\n",
    "    delta = 1\n",
    "    while delta > theta:\n",
    "        delta = 0\n",
    "        \"Looping over all the states i.e the money in hand for a current episode\"\n",
    "        for i in range(1,numStates):\n",
    "            oldvalue = value[i]\n",
    "            bellmanequation(i)\n",
    "            diff = abs(oldvalue-value[i])\n",
    "            delta = max(delta,diff)\n",
    "    print(value)\n",
    "\n",
    "def bellmanequation(num):\n",
    "    \"Initialize optimal value to be zero\"\n",
    "    optimalvalue = 0\n",
    "\n",
    "    \"The range of number of bets\"\n",
    "    for bet in range(0,min(num,100-num)+1):\n",
    "        \"Amount after winning and losing\"\n",
    "        win = num + bet\n",
    "        loss = num - bet\n",
    "        \"calculate the average of possible states for an action\"\n",
    "        \"In this case it would be home team winning or away team winning\"\n",
    "        sum = p * (reward[win] + gamma * value[win]) + (1 - p) * (reward[loss] + gamma * value[loss])\n",
    "\n",
    "        \"Choose the action that gives the max reward and update the policy and value for that\"\n",
    "        if sum > optimalvalue:\n",
    "            optimalvalue = sum\n",
    "            value[num] = sum\n",
    "            policy[num] = bet\n",
    "            \n",
    "reinforcement_learning() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 2, 3, 4, 5, 6, 0, 8, 9, 10, 0, 12, 0, 11, 10, 0, 8, 0, 0, 5, 4, 0, 2, 1, 0, 1, 2, 3, 4, 5, 6, 7, 0, 0, 10, 0, 12, 12, 11, 10, 9, 0, 0, 6, 5, 0, 3, 0, 0, 0, 1, 2, 3, 4, 5, 6, 43, 0, 0, 40, 0, 12, 37, 36, 35, 34, 0, 0, 6, 5, 0, 3, 0, 0, 0, 1, 2, 3, 0, 5, 6, 7, 8, 0, 10, 11, 0, 12, 11, 10, 9, 0, 7, 6, 5, 0, 3, 2, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taxi pick up and drop-off at right locations ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "Action Space Discrete(6)\n",
      "State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 328\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 428, -1, False)],\n",
       " 1: [(1.0, 228, -1, False)],\n",
       " 2: [(1.0, 348, -1, False)],\n",
       " 3: [(1.0, 328, -1, False)],\n",
       " 4: [(1.0, 328, -10, False)],\n",
       " 5: [(1.0, 328, -10, False)]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[328]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 4120\n",
      "Penalties incurred: 1221\n"
     ]
    }
   ],
   "source": [
    "env.s = 328  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 201\n",
      "State: 408\n",
      "Action: 2\n",
      "Reward: -1\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        #print(frame['frame'].getvalue())\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
